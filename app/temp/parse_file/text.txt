САНКТ-ПЕТЕРБУРГСКИЙ ГОСУДАРСТВЕННЫЙ УНИВЕРСИТЕТ
Факультет прикладной математики – процессов управления
М. Э. АББАСОВ
МЕТОДЫ ОПТИМИЗАЦИИ
Учебное пособие
Санкт-Петербург
2014
УДК 519.85
ББК 22.18
А 13
Р е ц е н з е н т ы : докт. физ.-мат. наук, проф. Л. Н. Полякова
(Санкт-Петербургский государственный университет),
докт.
физ.-мат.
наук,
проф.
В. И. Ерохин
(Санкт-
Петербургский государственный технологический ин-
ститут)
Печатается по постановлению Редакционно – издательского совета
факультета прикладной математики – процессов управления
Санкт-Петербургского государственного университета
А 13
Аббасов М. Э.
Методы оптимизации: Учеб. пособие / Аббасов М. Э.
— СПб.: Издательство “ВВМ”, 2014. — 64 с.
ISBN 978-5-9651-0875-6
Настоящее пособие содержит материалы практических заня-
тий курса “Методы оптимизации”, читаемого на факультете при-
кладной математики – процессов управления Санкт-Петербург-
ского государственного университета. Рассматриваются разделы:
одномерная и многомерная безусловная оптимизация, условная
оптимизация. В каждом разделе приведен необходимый минимум
теоретических сведений, разобраны примеры. Целью данного по-
собия является первоначальное знакомство читателя с алгоритма-
ми оптимизации, а также формирование у него понимания идей-
ных основ этих алгоритмов. Поэтому во многих местах пришлось
пожертвовать строгими обоснованиями и доказательствами для
как можно более простого, но интуитивно понятного изложения
материала.
Для студентов и аспирантов, специализирующихся в области
теории математического моделирования, оптимизации и матема-
тического программирования.
Библиогр. 10 назв.
УДК 519.85
ББК 22.18
Работа над пособием осуществлялась при финансовой поддержке
Российского Фонда Фундаментальных Исследований, грант №12-01-00752, а
также при поддержке гранта СПбГУ 9.38.205.2014.
ISBN 978-5-9651-0875-6
c⃝ М. Э. Аббасов, 2014
ОГЛАВЛЕНИЕ
Обозначения и условные знаки. . . . . . . . . . . . . . . . . . . . . . . . .
4
Глава 1. Вспомогательные сведения. . . . . . . . . . . . . . . . . . . .
5
§ 1.1.
Экстремум. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
5
Глава 2. Одномерная минимизация. . . . . . . . . . . . . . . . . . . .
7
§ 2.1.
Постановка задачи. Унимодальные функции.. . . . . . . .
7
§ 2.2.
Методы нулевого порядка.. . . . . . . . . . . . . . . . . . . . . . . . . .
7
§ 2.3.
Методы более высокого порядка.. . . . . . . . . . . . . . . . . . . .
16
Глава 3. Многомерная минимизация. . . . . . . . . . . . . . . . . . .
21
§ 3.1.
Метод покоординатного спуска. . . . . . . . . . . . . . . . . . . . . .
21
§ 3.2.
Градиентные методы. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
23
§ 3.3.
Ускоренные градиентные методы. . . . . . . . . . . . . . . . . . . .
29
§ 3.4.
Метод сопряженных направлений. . . . . . . . . . . . . . . . . . .
32
§ 3.5.
Метод Ньютона. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
39
§ 3.6.
Квазиньютоновский метод с поправкой ранга 1. . . . . .
41
§ 3.7.
Численный эксперимент. . . . . . . . . . . . . . . . . . . . . . . . . . . .
44
Глава 4. Условная оптимизация. . . . . . . . . . . . . . . . . . . . . . . .
47
§ 4.1.
Условия Куна-Таккера. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
47
§ 4.2.
Метод внешних штрафов. . . . . . . . . . . . . . . . . . . . . . . . . . .
50
§ 4.3.
Метод внутренних штрафов. . . . . . . . . . . . . . . . . . . . . . . .
56
§ 4.4.
Метод условного градиента. . . . . . . . . . . . . . . . . . . . . . . . .
59
Литература. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
64
ОБОЗНАЧЕНИЯ И УСЛОВНЫЕ ЗНАКИ
Rn — евклидово n-мерное пространство
intX — внутренность множества X
Bδ(y) — δ-окрестность точки y ({x: ||x − y|| ⩽ δ}))
1, k — множество натуральных чисел от 1 до k
A = [aij]n
i,j=1 — квадратная (n× n)-матрица с элементами aij
AT — транспонированная матрица A
x = (x1, . . . , xn)T — вектор-столбец
0 = (0, . . . , 0)∗ — нулевой вектор в Rn
f ′(x) — градиент функции f в точке x
f ′′(x) — матрица Гессе функции f в точке x
4
ГЛАВА 1.
ВСПОМОГАТЕЛЬНЫЕ СВЕДЕНИЯ
§ 1.1.
Экстремум
Пусть X – некоторое множество из Rn, и пусть задана непре-
рывная функция f : Rn → R.
Определение 1.1. Точку x∗ называют точкой глобального
минимума функции f(x) на множестве X , если f(x∗) ⩽ f(x) для
всех x ∈ X .
Определение 1.2. Точку x∗ называют точкой локального
минимума функции f(x) на множестве X , если существует такое
δ > 0, что f(x∗) ⩽ f(x) для всех x ∈ Bδ(x∗) � X .
Далее мы будем рассматривать задачу минимизации f(x) на
X , то есть задачу отыскания точек минимума функции f(x) на
множестве X . Если множество X замкнуто и ограничено, то по
теореме Вейерштрасса минимум на нем достигается.
Определение 1.3. Точку x∗ называют точкой глобального
максимума функции f(x) на множестве X , если f(x∗) ⩾ f(x) для
всех x ∈ X .
Определение 1.4. Точку x∗ называют точкой локального
максимума функции f(x) на множестве X , если существует такое
δ > 0, что f(x∗) ⩾ f(x) для всех x ∈ Bδ(x∗).
Из определений ясно, что любая точка глобального минимума
или максимума является так же и точкой локального минимума
или максимума соответственно, поэтому впредь, если не оговорено
иное, минимум будет пониматься в локальном смысле. Так как
max
x∈X f(x) = − max
x∈X(−f(x)),
то задача максимизации f(x) на X аналогична задаче минимиза-
ции −f(x) на X , поэтому ограничимся изучением лишь задач ми-
нимизации. Совокупность всех точек минимума и максимума функ-
ции на множестве X будем называть точками экстремума. Если в
вышеизложенных определениях заменить нестрогие неравенства на
строгие, то получим определения строгого экстремума.
5
Изучение начнем со случая X = Rn – поиска (локального)
минимума f на всем пространстве, то есть задачи безусловной ми-
нимизации.
Первая глава посвящена методам одномерной (f : R → R) без-
условной минимизации.
Далее во второй главе рассматриваются методы решения зада-
чи многомерной безусловной оптимизации, которые, по сути, сводят
задачу к последовательности задач одномерной минимизации.
Третья глава посвящена обсуждению задачи условной опти-
мизации (X ̸= Rn ). Здесь выводятся необходимые условия Куна-
Таккера и предлагаются методы сводящие, это задачу к последова-
тельности задач безусловной минимизации.
Список литературы включает 10 наименований.
6
ГЛАВА 2.
ОДНОМЕРНАЯ МИНИМИЗАЦИЯ
§ 2.1.
Постановка задачи. Унимодальные функции.
Для нахождения оптимума функции n переменных x1, . . . , xn
обычно используют итеративные методы, требующие на каждой
итерации решения оптимизационной задачи с одним переменным:
найти α > 0 минимизирующее функцию h(α) = f(x0 + αg), где
g – направление спуска, то есть направление вдоль которого наша
функция гарантированно убывает при достаточно малых α. Имен-
но поэтому изучение методов оптимизации начнем с самого просто-
го случая – минимизации функции скалярного аргумента, то есть
f : R → R.
Определение 2.1. Говорят, что функция f унимодальна на
действительном отрезке [a, b] если она имеет минимум x∗ ∈ [a, b] и
если для любых α, β ∈ [a, b] (α < β ) выполняются соотношения
f(α) > f(β) при β ⩽ x∗,
f(α) < f(β) при α ⩾ x∗.
Известный из курса математического анализа прием нахож-
дения решения уравнения f ′(x) = 0 на практике не всегда приме-
ним. Во-первых функция может быть недифференцируемой, а во-
вторых само решение указанного уравнения может оказаться весь-
ма сложным. В связи с этим особое значение приобретают методы
нулевого порядка, не требующие вычисления производной. Они,
как правило, применяются к унимодальным на рассматриваемом
отрезке функциям.
§ 2.2.
Методы нулевого порядка.
Далее будем рассматривать нашу функцию на фиксированном
отрезке [a, b].
Метод пассивного поиска. Это самый простой метод, пред-
лагающий разделить отрезок [a, b] на k равных частей точками
xi = a + b − a
k
i,
i = 0, k,
7
сравнить значения f(xi), i = 0, k и найти
xm : f(xm) = min
i=0,k
f(xi).
Ясно, что точка минимума так будет найдена с погрешностью
ε ⩽ b − a
k
.
Пример. Рассмотрим функцию f(x) = x+ 2
x , [a, b] = [0.5, 3.5],
и пусть нужно найти минимум с погрешностью ε ⩽ 1/2. Получаем
k ⩾ b−a
ε
= 6. Таким образом:
Номер индекса i
0
1
2
3
4
5
6
Значение аргумента xi
0.5
1
1.5
2
2.5
3
3.5
Значение функции f(xi)
4.5
3
2.83
3
3.3
3.67
4.07
Откуда xm = x2 = 1.5, а т.к. ε ⩽ 0.5, то для истинной точки мини-
мума можем гарантировать x∗ ∈ [1, 2], что верно, ведь x∗ =
√
2.
Отметим, что в этом методе все n + 1 точек выбираются
заранее и для более эффективного поиска минимума лучше
использовать методы последовательного поиска, в которых для
вычисления очередной точки xi используется информация, по-
лученная на более ранней стадии расчетов. К таким методам
относится метод половинного деления или дихотомии.
Метод дихотомии. В этом методе на каждой итерации бу-
дем получать отрезок, содержащий точку минимума (локализую-
щий отрезок), так, что длина каждого следующего отрезок меньше
длины предыдущего. Итак, вначале примем a1 = a, b1 = b, выбе-
рем некоторое положительное число δ > 0 и вычислим
c1 = a1 + b1
2
− δ
2, d1 = a1 + b1
2
+ δ
2.
Далее, получив значения функции f в точках c1 d1 , сравниваем
их.
Если f(c1) ⩽ f(d1), то a2 = a1, b2 = d1.
Если же f(c1) > f(d1), то a2 = c1 , b2 = b1.
8
Далее берем
c2 = a2 + b2
2
− δ
2, d2 = a2 + b2
2
+ δ
2
и опять сравниваем f(c2) с f(d2), определяя новые значения a3 ,
b3 и т.д. до тех пор, пока не выполнится εi = bi−ai
2
⩽ ε, где ε -
требуемая погрешность определения точки минимума.
Название метода связано с тем, что при малых δ длины лока-
лизующих интервалов на каждом шаге уменьшаются почти в два
раза. Понятно, что выбирая xm = ai+bi
2
получим точку минимума
с погрешностью εi = bi−ai
2
. Заметим, что с одной стороны выбор
достаточно малого δ гарантирует быстрое сокращение длины ло-
кализующего отрезка, но с другой стороны при этом приходится
вычислять значение функции с большей точностью (большим ко-
личеством значащих знаков после запятой), так как иначе можно
«пропустить» точку минимума. В рамках изложенного алгоритма
это означает, что возможна ситуация, когда точка минимума распо-
ложена в интервале (di, bi), но из-за недостаточно точного вычис-
ления значения функции в точках ci , di принимается f(ci) = f(di),
в то время, как на самом деле f(ci) > f(di). Таким образом, новый
локализующий отрезок имеет вид [ai, di] и, очевидно, не содержит
точку минимума.
а)
б)
Рисунок 2.1: Определение локализующего отрезка для унимодаль-
ной функции (штриховкой показана отбрасываемая часть отрезка)
9
Выгода рассмотренного подхода очевидна – в отличие от пас-
сивного поиска, в методе дихотомии не требуется вычислять значе-
ния функции в точках, принадлежащих отбрасываемым на каждом
шаге полуинтервалам.
Пример. Рассмотрим функцию f(x) = x+ 2
x , a = 0.5, b = 3.5
и найдем точку минимума с погрешностью ε = 0.5. Пусть δ = 0.1,
тогда
1. a1 = 0.5, b1 = 3.5,
c1 = 0.5+3.5
2
− 0.1
2 = 1.95, d1 = 0.5+3.5
2
+ 0.1
2 = 2.05,
f(c1) = 2.976 < f(d1) = 3.026;
2. a2 = a1 = 0.5, b2 = d1 = 2.05, ε2 = 2.05−0.5
2
= 0.775 > 0.5,
поэтому продолжаем.
c2 = 0.5+2.05
2
− 0.1
2 = 1.225, d2 = 0.5+2.05
2
+ 0.1
2 = 1.325,
f(c2) = 2.858 > f(d2) = 2.834;
3. a3 = c2 = 1.225, b3 = d2 = 2.05, ε3 = 2.05−1.225
2
= 0.4125 < 0.5,
следовательно
это
последняя
итерация.
Принимаем
xm = 2.05+1.225
2
= 1.638.
Заметим, что метод дихотомии позволил для получения точ-
ки минимума с заданной погрешностью обращаться к вычислению
значения функции четыре раза, вместо семи в методе пассивного
поиска.
Задания
1. В изложенном алгоритме при f(ci) = f(di) предлагается
принять ai+1 = ai , bi+1 = di . Какие есть еще варианты по-
строения локализующего отрезка [ai+1, bi+1] в этом случае?
2. Показать, что погрешность εi = b − a − δ
2i
+ δ
2
Метод золотого сечения. Более обдуманный подход к про-
цессу поиска точки минимума позволил сэкономить на количе-
стве обращений к вычислению функции. Однако в методе дихо-
томии имеется особенность, позволяющая строить более эффек-
тивные алгоритмы. На каждой итерации приходится вычислять
10
значение функции в двух новых точках ci и di . Если удаст-
ся построить алгоритм так, чтобы каждый раз одна из этих то-
чек совпадала с одной из аналогичных точек с предыдущей ите-
рации, т.е. ci−1 или di−1 , то это бы позволило на каждой ите-
рации вычислять значение функции лишь в одной новой точке
и, тем самым, обращаться к вычислению функции еще меньше.
Рисунок 2.2: Золотое се-
чение
Оказывается, это можно сделать с по-
мощью деления отрезка в пропорциях
золотого сечения. Напомним, что от-
резок AB (см. рис. 2.2) разделен точ-
кой D в пропорции золотого сечения,
если отношение всей длины отрезка к
длине большей его части равно отноше-
нию длины большей его части к длине меньшей, т.е.
AB
AD = AD
DB .
Пусть длина AB = 1, а AD = x. Тогда
1
x =
x
1 − x,
откуда
x
=
√
5−1
2
.
Понятно,
что
больший
отрезок
мож-
но
было
бы
отложить
не
от
левого,
а
от
правого
кон-
ца отрезка. Тогда получили бы точку золотого сечения
C ,
симметричную
т.
D
относительно центра,
и
AC
=
3−
√
5
2
.
Рисунок 2.3: Первая и вторая точ-
ка золотого сечения
Точку C называют первой, а
D второй точкой золотого се-
чения. Эти точки обладают за-
мечательными свойствами (см.
рис. 2.3):
Точка C не только первая точка золотого сечения отрезка
AB, но и вторая точка золотого сечения для отрезка AD;
точка D не только вторая точка золотого сечения отрезка AB,
но и первая точка золотого сечения отрезка CB.
Поэтому использование точек золотого сечения для определе-
ния границ локализующих отрезков позволяет на каждой итерации
11
вычислять значение функции лишь в одной новой точке.
Итак, опишем сам алгоритм.
На первой итерации принимаем a1 = a, b1 = b и вычисляем
c1 = 3 −
√
5
2
(b1 − a1) + a1, d1 =
√
5 − 1
2
(b1 − a1) + a1.
Далее, получив значения функции f в точках c1 d1 , сравниваем
их.
Если f(c1)
⩽
f(d1), то a2
=
a1, b2
=
d1, d2
=
c1 ,
c2 = 3−
√
5
2
(b2 − a2) + a2;
Если же f(c1) > f(d1), то a2 = c1, b2 = b1, c2 = d1 ,
d2 =
√
5−1
2
(b2 − a2) + a2 .
Далее сравниваем f(c2) с f(d2), определяя новые значения
a3, b3, и т.д. до тех пор, пока не выполнится εi =
bi−ai
2
⩽ ε,
где ε - требуемая точность. На каждой итерации длина локализу-
ющего отрезка уменьшается в
√
5−1
2
раз, следовательно
εi = 1
2
�√
5 − 1
2
�i−1
(b − a).
Пример. Рассмотрим ту же функцию
f(x) = x + 2
x,
a = 0.5, b = 3.5 и найдем точку минимума с погрешностью ε = 0.5.
1. a1 = 0.5, b1 = 3.5,
c1 = 3−
√
5
2
(3.5 − 0.5) + 0.5 = 1.646,
d1 =
√
5−1
2
(3.5 − 0.5) + 0.5 = 2.354,
f(c1) = 2.861 < f(d1) = 3.204;
2. a2 = a1 = 0.5, b2 = d1 = 2.354, d2 = c1 = 1.646,
c2 = 3−
√
5
2
(b2 − a2) + a2 = 1.208, ε2 = 2.354−0.5
2
= 0.927 > 0.5,
поэтому продолжаем.
f(c2) = 2.864, f(d2) = f(c1) = 2.861,
f(c2) > f(d2);
12
3. a3 = c2 = 1.208, b3 = b2 = 2.354, c3 = d2 = 1.646,
d3 =
√
5−1
2
(b3 − a3) + a3 = 1.916
ε2 = 2.354−1.208
2
= 0.573 > 0.5, поэтому продолжаем.
f(c3) = f(d2) = 2.861, f(d3) = 2.96,
f(c3) < f(d3);
4. a4 = a3 = 1.208, b4 = d3 = 1.916, d4 = c3 = 1.646,
c4 = 3−
√
5
2
(b4 − a4) + a4 = 1.478,
ε2 = 1.916−1.208
2
= 0.354 < 0.5, т.о. это последняя итерация.
Принимаем xm = 1.916+1.208
2
= 1.562.
Задания
1. Почему первая точка золотого сечения отрезка AB (см. рис.
2.3) является второй точкой золотого сечения для отрезка
AD?
2. Почему вторая точка золотого сечения отрезка AB (см.
рис. 2.3) является первой точкой золотого сечения для
отрезка CB?
Метод Фибоначчи. Обозначенную в начале предыдущего
пункта идею можно реализовать также с помощью чисел Фибо-
наччи. Напомним, как они определяются:
F1 = 1, F2 = 1,
Fn+2 = Fn + Fn+1, n ⩾ 1.
Приняв
c1 = a1 + (b1 − a1) Fn
Fn+2
,
d1 = a1 + (b1 − a1)Fn+1
Fn+2
,
где n– некоторое натуральное число, большее единицы, получим
две симметричные относительно центра отрезка точки. Действи-
тельно, они равноудалены от концов отрезка, так как
c1 − a1 = (b1 − a1) Fn
Fn+2
= (b1 − a1)
�
1 − Fn+1
Fn+2
�
= b1 − d1.
13
Далее, сравнив значения f(c1) с f(d1) нужно отбросить один из
полуинтервалов [a1, c1) или (d1, b1].
Если f(c1) ⩽ f(d1) отбрасываем (d1, b1]. Точку c1 принад-
лежащая новому локализующему отрезку [a2, b2] = [a1, d1] будет
делить его в соотношении
c1 − a1
d1 − a1
=
Fn
Fn+2
Fn+1
Fn+2
=
Fn
Fn+1
.
Поэтому в этом случае d2 = a2 + (b2 − a2) Fn
Fn+1 = c1.
Если f(c1) > f(d1) отбрасываем [a1, c1). Точку d1 принад-
лежащая новому локализующему отрезку [a2, b2] = [c1, b1] будет
делить его в соотношении
d1 − c1
b1 − c1
= d1 − c1
d1 − a1
=
Fn+1
Fn+2 −
Fn
Fn+2
Fn+1
Fn+2
= Fn−1
Fn+1
.
Поэтому в этом случае c2 = a2 + (b2 − a2) Fn−1
Fn+1 = d1 .
Таким образом положив a1 = a, b1 = b,
ci = ai + (bi − ai)Fn+1−i
Fn+3−i
,
di = ai + (bi − ai)Fn+2−i
Fn+3−i
,
на каждой итерации опять будем вычислять значение функции
только в одной новой точке. Кроме того, так как
b2 − a2 = (b1 − a1)Fn+1
Fn+2
,
b3 − a3 = (b2 − a2) Fn
Fn+1
= (b1 − a1)Fn+1
Fn+2
Fn
Fn+1
= (b1 − a1) Fn
Fn+2
,
и вообще
bi − ai = (b1 − a1)Fn+3−i
Fn+2
,
то
bn − an = (b1 − a1) F3
Fn+2
.
14
Отсюда получаем, что для определения решения с погрешностью,
не превосходящей ε, нужно n выбирать из условия
(b − a) F3
Fn+2
⩽ 2ε,
т.е
Fn+2 ⩾ (b − a)
ε
.
(2.1)
Пример. Возьмем функцию f(x) = x + 2
x , a = 0.5, b = 3.5 и
найдем точку минимума с погрешностью ε = 0.5. Из условия (2.1)
Fn+2 ⩾ 6, откуда n ⩾ 4.
1. Итак, примем n = 4, a1 = 0.5, b1 = 3.5,
c1 = 0.5 + (3.5 − 0.5) F4
F6 = 0.5 + 3 3
8 = 1.625,
d1 = 0.5 + (3.5 − 0.5) F5
F6 = 0.5 + 3 5
8 = 2.375,
f(c1) = 2.856 < f(d1) = 3.217;
2. a2 = a1 = 0.5, b2 = d1 = 2.375, d2 = c1 = 1.625,
c2 = 0.5 + (2.375 − 0.5) F3
F5 = 0.5 + 1.875 3
5 = 1.25,
f(c2) = 2.85, f(d2) = f(c1) = 2.856,
f(c2) < f(d2);
3. a3 = a2 = 0.5, b3 = d2 = 1.625, d3 = c2 = 1.25,
c3 = 0.5 + (1.625 − 0.5) F2
F4 = 0.5 + 1.125 1
3 = 0.875
f(c3) = 3.161, f(d3) = f(c2) = 2.85,
f(c3) > f(d3);
4. a4 = c3 = 0.875, b4 = b3 = 1.625, c4 = d3 = 1.25
d4 = 0.875 + (1.625 − 0.875) F2
F3 = 0.875 + 0.75 1
2 = 1.25.
Принимаем xm = c4 = d4 = 1.625 + 0.875
2
= 1.25 и завершаем
расчеты. Как видим, ε2 = 1.625 − 0.875
2
= 0.375 < 0.5.
15
В отличии от методов дихотомии и золотого сечения в методе
Фибоначчи не нужно на каждом шаге проверять критерий останов-
ки (условие bi−ai ⩽ 2ε). Количество шагов здесь задается заранее,
исходя из (2.1).
Отметим также, что метод Фибоначчи является наиболее оп-
тимальным, в том смысле, что позволяет найти решение с заданной
точностью при наименьшем количестве обращений к вычислению
функции. Это может быть важно, когда вычисление функции свя-
зано с дорогостоящими физическими опытами либо сама функция
имеет сложный вид, что делает ее вычисление трудоемким. Так, в
рассмотренных примерах (отыскания точки минимума одной и той
же функции с одной и той же точностью) при применении мето-
да дихотомии пришлось вычислять значение функции шесть раз,
метода золотого сечения – пять раз, а метода Фибоначчи – всего
четыре раза.
Задания
1. Какой результат будет получен, если применить метод
дихотомии, золотого сечения или Фибоначчи к функциям,
не являющимся унимодальными на рассматриваемом от-
резке?
§ 2.3.
Методы более высокого порядка.
Идеи всех методов, которые будут рассматриваться в данном
пункте основаны на поиске стационарных точек, т.е. таких точек,
в которых выполнено необходимое условие минимума
Теорема 2.1. Пусть f(x) гладкая функция, заданная на Rn,
тогда если x∗ – т. экстремума, то f ′(x∗) = 0.
Метод касательных. Начнем с метода первого порядка, то
есть такого, который использует производную рассматриваемой
функции.
Определение 2.1. Функция f : Rn → R называется выпук-
лой на отрезке [a, b], если для любого α ∈ [0, 1] и любых x1 , x2 из
16
[a, b] выполняется
f
�
αx1 + (1 − α)x2
�
⩽ αf(x1) + (1 − α)f(x2).
Пусть функция f(x) выпукла и дифференцируема на отрезке
[a, b]. Если f ′(a) ⩾ 0 или f ′(b) ⩽ 0, то т. a либо т. b является
точкой минимума соответственно, и тогда задача решена, поэтому
пусть f ′(a) < 0, f ′(b) > 0. Примем a1 = a, b1 = b.
Рисунок
2.4:
Геометриче-
ская
иллюстрация
метода
касательных.
Далее строим касательные к
графику f(x) в точках a1 и b1 и на-
ходим точку их пересечения. Обозна-
чим ее c1.
Если f ′(c1) > 0, то принимаем
a2 = a1, b2 = c1.
Если f ′(c1) < 0, то принимаем
a2 = c1, b2 = b1.
Если f ′(c1) = 0, то задача ре-
шена и точка c1 – искомая.
Повторяем процедуру до тех
пор пока не выполнится критерий
остановки, в качестве которого тут можно взять, например,
|bk − ak| ⩽ ε
(2.2)
или
|f ′(ck)| ⩽ ε,
(2.3)
или
|f(bk) − f(ak)| ⩽ ε,
(2.4)
либо их комбинацию. Здесь ε – некоторое положительное число.
На рис. 2.4 показано три итерации метода касательных. Ясно,
что реализация этого метода основывается, по сути, на приближе-
нии графика нашей функции кусочно-линейной аппроксимацией.
Задания
1. Подумайте над различиями критериев остановки 2.2 и 2.3.
Приведите примеры графиков функций, для которых 2.2 вы-
полнится раньше 2.3 и наоборот.
17
2. Зная ak , bk , f(x) получите выражение для ck .
Метод
Ньютона-Рафсона. Нашей целью является по-
иск решения уравнения f ′(x) = 0. Можно решать эту задачу
так: задаем x0 , проводим в этой точке касательную к функции
f ′(x), находим точку пересечения этой касательной с осью Ox.
Рисунок
2.5:
Геометрическая
иллюстрация
метода
Ньютона-
Рафсона.
Обозначаем эту т. x1 . Продол-
жаем процедуру до тех пор
пока не выполнится критерий
остановки f ′(xk) ⩽ ε, для неко-
торого положительного ε.
Очевидно ордината точек
касательной описывается урав-
нением
f ′(xk) + f ′′(xk)(x − xk),
поэтому приравняв это выра-
жение к нулю, найдем xk+1
xk+1 = xk − f ′(xk)
f ′′(xk).
Рассматриваемый алгоритм является методом второго поряд-
ка.
Пример. Рассмотрим функцию
f(x) = x + 2
x,
выберем x0 = 0.5 и найдем точку минимума с погрешностью
ε = 0.5.
Имеем
f ′(x) = 1 − 2
x2 , f ′′(x) = 4
x3 .
1. f ′(x0) = −7, |f ′(x0)| > ε, поэтому продолжаем вычисления
f ′′(x0) = 32, x1 = 0.5 − −7
32 = 0.719,
18
2. f ′(x1) = −2.869, |f ′(x1)| > ε, продолжаем вычисления
f ′′(x1) = 10.762, x1 = 0.719 − −2.869
10.762 = 0.986,
3. f ′(x1) = −1.057, |f ′(x1)| > ε, продолжаем вычисления
f ′′(x1) = 4.173, x2 = 0.986 − −1.057
4.173 = 1.239,
4. f ′(x2) = −0.303, |f ′(x1)| < ε, поэтому вычисления заканчи-
ваем и принимаем
xm = x2 = 1.239.
Задания
1. Метод Ньютона-Рафсона чувствителен к выбору начальной
точки. Приведите пример, когда из-за неудачно выбранной
начальной точки метод расходится.
Метод секущих. Если в методе Ньютона-Рафсона вместо
Рисунок 2.6: Геометрическая
иллюстрация метода секущих.
второй
производной
воспользо-
ваться ее приближением
f ′′(xk) ≈ f ′(xk) − f ′(xk−1)
xk − xk−1
,
то получим метод секущих, или,
как его еще называют, метод хорд,
являющийся методом первого по-
рядка
xk+1 = xk−
xk − xk−1
f ′(xk) − f ′(xk−1)f ′(xk).
Как видим, выражение описывает точку пересечения прямой, про-
ходящей через точки (xk, f ′(xk)) и (xk−1, f ′(xk−1)).
Задания
1. Показать справедливость последнего утверждения.
19
В заключении главы отметим, что чем больше информации
об исследуемой функции в процессе минимизации используется,
тем быстрее достигается результат. Таким образом, скорость схо-
димости процесса минимизации увеличивается, но попутно возрас-
тает количество необходимых вычислений. Так, из рассмотренных
методов, медленнее всего сходятся методы нулевого прядка, далее
идут методы первого, а затем второго порядка. Однако, например,
в методе касательных, в отличие от метода золотого сечения, при-
ходится вычислять не только значение рассматриваемой функции,
но и ее производную, а в методе Ньютона-Рафсона еще и вторую
производную.
20
ГЛАВА 3.
МНОГОМЕРНАЯ МИНИМИЗАЦИЯ
Пусть теперь f : Rn → R, то есть имеем дело с функци-
ей n переменных и нужно по-прежнему найти точку минимума.
§ 3.1.
Метод покоординатного спуска
Здесь минимизация осуществляется циклами, по n шагов в каж-
дом. Пусть xk = (xk
1, xk
2, . . . , xk
n)T .
• Фиксируем значения xk
2, . . . , xk
n и проводим минимизацию по
переменной x1 . Получаем (xk+1
1
, xk
2, . . . , xk
n)T , на этом первый
шаг окончен.
• Фиксируем xk+1
1
, xk
3 . . . , xk
n и проводим минимизацию по x2 .
Получаем (xk+1
1
, xk+1
2
, xk
3, . . . , xk
n)T .
• · · ·
· · ·
· · ·
• Фиксируем xk+1
1
, xk+1
2
, . . . , xk+1
n−1 и проводим минимизацию по
xn . Получаем (xk+1
1
, xk+1
2
, . . . , xk+1
n
)T
Рисунок 3.1: Иллюстрация метода покоординатного спуска.
Итак, после n шагов, соответствующих каждой из переменных
завершаем первый цикл, получая xk+1 = (xk+1
1
, xk+1
2
, . . . , xk+1
n
)T .
Процесс
повторяется
до
тех
пор,
пока
для
некоторого
за-
данного положительного
ε
не выполнится условие остановки
|f(xk+1) − f(xk)| ⩽ ε или ||xk+1 − xk|| ⩽ ε, либо их комбинация.
21
Рисунок 3.2: Метод «застревает» в точке, отличной от оптимума.
Отметим, что на каждом шаге цикла приходится решать за-
дачу одномерной минимизации, которой была посвящена первая
глава.
Рисунок 3.3: Линии уровня.
Иллюстрируют работу методов многомерной оптимизации
обычно с помощью линий уровня – геометрического места то-
чек пространства аргументов, для которых значения исследуемой
функции одинаковы (см. рис. 3.3). Для функций двух переменных
линии уровня изображаются на плоскости, что позволяет наглядно
продемонстрировать работу алгоритма. Так на рис. 3.1 видно, что
в методе покоординатного спуска минимизация ведется поочеред-
но по направлениям параллельным осям координат (по ортам). На
рис. 3.2 показан случай, когда метод останавливается ("застрева-
22
ет") в точке x1 , не являющийся оптимумом – любое движение из
нее вдоль ортов приводит лишь к увеличению значения функции.
Пример. Рассмотрим функцию f(x) = 9x2
1 + x2
2, выберем
x0 = (1, 1)T и найдем точку минимума с погрешностью ε = 0.5,
взяв в качестве критерия остановки условие ||xk+1 − xk|| ⩽ ε
1.
(a) Фиксируем x2 = x0
2 = 1 и проводим минимизацию по
переменной x1 . Таким образом, нужно минимизировать
функцию f(1 + α, 1) = 9(1 + α)2 + 1 по скалярному ар-
гументу α > 0, то есть найти оптимальный в смысле
минимизации функции шаг из т. x0 вдоль орта (1, 0)T .
Очевидно α = −1, поэтому x1
1 = 0.
(b) Фиксируем x1 = x1
1 = 0 и проводим минимизацию по
переменной x2 . То есть нужно минимизировать функцию
f(0, 1+α) = (1+α)2 по α > 0. Очевидно α = −1, поэтому
x1
2 = 0.
Окончательно после первого цикла получаем x1 = (0, 0)T , но
||x1 − x0|| =
√
2 ⩽ 0.5, поэтому вычисления продолжаем.
2.
(a) Аналогично предыдущему получаем x2
1 = 0,
(b) x2
2 = 0.
Итак, имеем x2 = (0, 0)T , но ||x2 − x1|| = 0 < 0.5, поэтому
вычисления останавливаем и принимаем xmin = (0, 0)T .
§ 3.2.
Градиентные методы
Вначале рассмотрим идейную основу.
Выпишем разложение функции f в окрестности точки xk в
ряд Тейлора:
f(x) = f(xk) + ⟨f ′(xk), x − xk⟩ + o(x − xk),
(3.1)
где f ′(xk) – градиент функции f в точке xk , а
lim
x→xk
o(x − xk)
∥x − xk∥ = 0.
23
Ясно,
что
при
достаточно
малом
∥x − xk∥
значение
раз-
ности
f(x) − f(xk)
определяется главной частью приращения
⟨f ′(xk), x − xk⟩. С другой стороны, скалярное произведение двух
векторов принимает наименьшее значение тогда, когда векторы
разнонаправлены. Поэтому при x − xk = −αf ′(xk), где α > 0 –
достаточно малое положительное число, получим
f(x)−f(xk) ≈ ⟨f ′(xk), x−xk⟩ = ⟨f ′(xk), −αf ′(xk)⟩ = −α∥f ′(xk)∥2 < 0.
Поэтому приняв xk+1 = xk−αf ′(xk), при том же самом α, получим
f(xk+1) < f(xk) и сможем таким образом построить последователь-
ность {xk}, на которой значения нашей функции будут убывать.
Число α представляет собой длину шага, который необходимо сде-
лать в направлении антиградиента.
Существует несколько вариантов этого метода, отличающиеся
способом выбора длины шага.
Градиентный метод с дроблением шага. Этот метод
предложен и подробно рассматривается в [2]. Вначале выбирают
некоторое �α > 0 и произвольное число ε ∈ (0, 1). Эти значения
одни и те же для всех итераций (k = 1, 2, . . . ). Далее принимают
α = �α и
1. определяют точку x = xk − αf ′(xk);
2. вычисляют f(x) = f(xk − αf ′(xk));
3. проверяют выполнение неравенства
f(x) − f(xk) ⩽ −αε∥f ′(xk)∥2;
4. если это неравенство выполняется, то принимают αk = α, ес-
ли же неравенство не выполняется, производим деление α
путем умножения на произвольное положительное λ < 1 и
переходим к шагу 1;
5. получают xk+1 = xk −αkf ′(xk), проверяют критерий останов-
ки (например ∥f ′(xk+1)∥ < δ, для некоторого малого δ > 0)
и переходят к следующей итерации.
24
Как видим, значение функции на каждом шаге меньше, чем на
предыдущем. Однако при приближении к точке экстремума, нор-
ма градиента становится мала и, соответственно (см. неравенство
из третьего пункта алгоритма), значение функции уменьшается на
малую величину.
Таким образом, на последовательности {xk} функция убыва-
ет, но это убывание становится все медленнее по мере приближения
к точке экстремума.
У читателя может возникнуть вопрос о том, какие условия
гарантируют выполнение проверяемого неравенства при малых α.
Ответ на этот вопрос получим в ходе доказательства следующей
теоремы.
Теорема 3.1. Если гладкая функция f(x) ограничена снизу,
ее градиент f ′(x) удовлетворяет условию Липшица:
|f ′(x) − f ′(y)| ⩽ L∥x − y∥,
∀x, y ∈ Rn,
а выбор значения αk производится по описанному выше алгорит-
му, то ∥f ′(xk)∥ → 0 при k → ∞, какова бы ни была начальная
точка x0 .
Доказательство. По теореме о среднем имеем
f(x) − f(xk) = ⟨f ′(xkc), x − xk⟩,
где xkc = xk + θ(x − xk), θ ∈ [0, 1]. Далее
f(x) − f(xk) = ⟨f ′(xkc) − f ′(xk), x − xk⟩ + ⟨f ′(xk), x − xk⟩ ⩽
⩽ L∥xkc − xk∥∥x − xk∥ − α∥f ′(xk)∥2 = α∥f ′(xk)∥2(αL − 1).
Если выбирать такие малые α, чтобы
α∥f ′(xk)∥2(αL − 1) ⩽ −εα∥f ′(xk)∥2,
то есть
α ⩽ �α = 1 − ε
L
,
то выполнится и проверяемое в алгоритме неравенство. Тем са-
мым показано, что требуемое неравенство действительно выпол-
нится при достаточно малых α.
25
Итак, выбирая αk в соответствии с приведенным выше алго-
ритмом, для всех k = 1, 2, . . . (при ∥f ′(xk)∥ ̸= 0) получим
f(xk+1) − f(xk) ⩽ −εα∥f ′(xk)∥2 < 0.
Таким образом, последовательность {f(xk)} строго монотонно
убывает и ограничена снизу, поэтому она сходится, а, следователь-
но, является фундаментальной. Отсюда f(xk+1) − f(xk) → 0, при
k → ∞. Так как при любых α меньших �α проверяемое неравенство
гарантированно выполнится, можно утверждать, что при реализа-
ции алгоритма будем получать α по крайней мере не меньше �α, то
есть α ⩾ �α, откуда
∥f ′(xk)∥ ⩽ f(xk) − f(xk+1)
εα
⩽ f(xk) − f(xk+1)
ε�α
.
Числа ε, �α не зависят от k, поэтому ∥f ′(xk)∥ → 0 при k → ∞.
Градиентный метод с постоянным шагом. Из доказа-
тельства теоремы 3.1 видно, что если при всех k положить αk = α,
где α – некоторое фиксированное число, для которого
0 < α ⩽ 1 − ε
L
,
то соответствующая последовательность {xk} будет сходиться к
стационарной точке. Таким образом, приходим к варианту гради-
ентного метода с постоянным шагом. Для его реализации выбирают
некоторое положительное число α (на практике обычно не превос-
ходящее 0.01), а затем с помощью известного xk , строят
xk+1 = xk − αf ′(xk),
не забывая, опять же, на каждой итерации проверять критерий
остановки.
Градиентный спуск с заранее заданным шагом. Здесь
предлагается выбрать в качестве αk соответствующие значения из
последовательности {λk}, для которой





λk, λk → 0 при k → ∞,
∞
�
k=1
λk = ∞.
26
Так, можно положить αk = 1
k , и тогда
xk+1 = xk − 1
k f ′(xk).
Этот метод называют так же методом расходящегося ряда.
Метод наискорейшего градиентного спуска (МНГС).
Можно искать оптимальную длину шага, то есть αk , доставляющее
минимум функции в направлении антиградиента:
f(xk − αkf ′(xk)) = min
α⩾0 f(xk − αf ′(xk)).
Заметим, что по необходимому условию экстремума оптимальное
α должно удовлетворять
d
dαf(xk − αf ′(xk)) = 0,
то есть
⟨−f ′(xk), f ′(xk − αf ′(xk))⟩ = 0.
Определив отсюда αk , получим:
⟨−f ′(xk), f ′(xk+1⟩ = 0,
то есть направления, вдоль которых ведется минимизация в методе
наискорейшего градиентного спуска на соседних шагах ортогональ-
ны.
Если рассматриваемая функция выпукла, то ее линии уровня
схематически можно изобразить так, как это сделано на рис. 3.4.
Ясно, что «большей» линии уровня соответствует большее значе-
ние функции. Поэтому делая из точки xk оптимальный шаг вдоль
полупрямой, определяемой антиградиентом, попадаем в точку ка-
сания этой полупрямой с некоторой линией уровня. Действительно,
через любую другую точку этой полупрямой проходит «б´ольшая»
линия уровня, а, значит, и значение функции в любой другой точке
больше.
Градиент функции в точке ортогонален ее линии уровня, про-
ходящей через ту же точку. Отсюда опять приходим к уже извест-
ному факту: направления, вдоль которых ведется минимизация в
методе наискорейшего градиентного спуска на соседних шагах, ор-
тогональны.
27
Рисунок 3.4: Метод наискорейшего градиентного спуска.
Пример. Рассмотрим функцию f(x) = 9x2
1 + x2
2, выберем
x0 = (1, 1) и найдем точку минимума с погрешностью ε = 0.05,
взяв в качестве критерия остановки условие ∥f ′(xk)∥ ⩽ ε. Будем
вести расчеты с точностью до третьего знака после запятой. Имеем
f ′(x1, x2) = (18x1, 2x2)T , поэтому
1. x1 = x0 − αf ′(x0) = (1, 1)T − α(18, 2)T, откуда
f(x0 − αf ′(x0)) = 9(1 − 18α)2 + (1 − 2α)2.
Поэтому
α1 = arg min
α>0 f(x0 − αf ′(x0)) = 0.056,
x1 = x0 − α0f ′(x0) = (−0.008, 0.888)T.
2. f ′(x1) = (−0.144, 1.776)T . Критерий остановки не выполнен
∥f ′(x1)∥ =
��∂f(x1)
∂x1
�2
+
�∂f(x1)
∂x2
�2
> ε,
продолжаем вычисления. Действуя аналогично предыдуще-
му, получим α1 = 0.475, x2 = (0.06, 0.044)T . Далее
3. f ′(x2)
=
(1.087, 0.089)T ,
∥f ′(x2)∥
>
ε,
α2
=
0.056,
x3 = (0.000, 0.039)T .
28
4. f ′(x3)
=
(0.000, 0.078)T ,
∥f ′(x3)∥
>
ε,
α2
=
0.456,
x4 = (0.000, 0.003)T .
5. f ′(x4) = (0.000, 0.006)T , ∥f ′(x4)∥ < ε, поэтому принимаем в
качестве точки минимума xmin = x4 .
В градиентных методах с постоянным и заранее заданным ша-
гом расстояние, проходимое на каждой итерации вдоль антигради-
ента мало, поэтому они сходятся медленно. В методе же наискорей-
шего градиентного спуска с помощью решения задачи одномерной
минимизации, находится оптимальная длина шага, что ведет к бо-
лее быстрой сходимости.
Задания
1. Проанализируйте на примере, как влияет выбор чисел �α, ε,
λ на сходимость градиентного метода с дроблением шага.
2. Исходя из разложения (3.1) ответьте, почему ε в градиент-
ном методе с дроблением шага строго меньше единицы.
3. Показать, что градиент функции в некоторой точке орто-
гонален ее линии уровня, проходящей через ту же точку.
§ 3.3.
Ускоренные градиентные методы
Возможны случаи, когда линии уровня исследуемой функции
сильно вытянуты в одном направлении и сплющены в другом. В
этом случае говорят, что функция «овражного» типа. На этом
классе функций обычные градиентные методы сходятся плохо.
Например, на рис. 3.5 показана работа метода наискорейшего
градиентного спуска при минимизации овражной функции. Видно,
что быстро опустившись на дно оврага метод начинает медленное
движение к точке минимума по «пилообразной» траектории. Для
ускорения сходимости в подобных случаях используют специ-
альные приемы, дающие возможность получить направление,
проходящее вдоль «дна оврага». Это дает возможность сделать
большой шаг по направлению к точке минимума и таким образом
быстро приблизиться к ней.
29
Рисунок 3.5: Медленная сходимость метода наискорейшего гради-
ентного спуска.
Ускоренный градиентный метод p-го порядка. Пусть
получена точка xk . Предлагается [3] из этой точки сделать p шагов
по методу наискорейшего градиентного спуска, дойдя до точки yk .
Далее для получения xk+1 проводится одномерная минимизация в
направлении yk − xk с началом в xk . На практике рекомендуется
брать p = n, где n – размерность пространства (x-ов).
Рисунок 3.6: Ускоренный градиентный спуск второго порядка.
Отметим, что здесь для получения очередной точки последо-
вательности {xk} приходится решать n+ 1 задачу одномерной ми-
нимизации. То есть каждая итерация рассматриваемого метода бо-
лее дорогостоящая, чем итерация по МНГС.
30
Пример. Рассмотрим функцию f(x) = 9x2
1 + x2
2, выберем
x0 = (1, 1) и найдем точку минимума с погрешностью ε = 0.05,
взяв в качестве критерия остановки условие ∥f ′(xk)∥ ⩽ ε.
Сделав из x0 = (1, 1)T два шага по методу наискорейшего
градиентного спуска (см. пример 3.2), получим y0 = (0.06, 0.044)T .
Совершаем одномерную минимизацию из точки x0 вдоль направ-
ления y0 − x0 = (−0.94, −0.956)T .
f(x0 + α(y0 − x0)) = 9(1 − 0.94α)2 + (1 − 0.956α)2 = ϕ(α),
откуда
argmin
α
ϕ(α) = 1.062,
и, соответственно,
x1 = (1 − 0.94 · 1.062, 1 − 0.956 · 1.062)T = (0.002, −0.015)T.
Так как ∥f ′(x1)∥ = 0.043 ⩽ ε, то принимаем
xmin = (0.002, −0.015)T.
По сравнению с МНГС в ходе реализации данного алгоритма
пришлось решить на одну задачу одномерной оптимизации меньше.
Овражный метод. Пусть получена точка xk . В этом алго-
Рисунок 3.7: Овражный метод.
ритме [4] берется точка �xk , близкая к xk . Из этих точек делается
31
несколько шагов по методу градиентного спуска. В результате по-
лучаются точки yk , �yk , определяющие прямую, проходящую почти
вдоль «дна оврага». Далее для получения xk+1 проводится одно-
мерная минимизация в направлении �yk − yk с началом в yk .
Пример. Рассмотрим функцию f(x) = 9x2
1 + x2
2, выберем
x0 = (1, 1)T и найдем точку минимума с погрешностью δ = 0.05,
взяв в качестве критерия остановки условие ∥f ′(xk)∥ ⩽ ε.
Выберем �x0 = (1.1, 1.1)T . Сделав из x0 (см. пример 3.2) и
�x0 по одному шагу по методу наискорейшего градиентного спус-
ка, получим y0 = (−0.008, 0.888)T , �y0 = (−0.009, 0.977)T . Совер-
шаем одномерную минимизацию из точки y0 вдоль направления
�y0 − y0 = (−0.001, 0.089)T .
f(y0 + α(�y0 − y0)) = 9(−0.008 − 0.001α)2 + (0.888 + 0.089α)2 = ϕ(α),
откуда
argmin
α
ϕ(α) = 9.975,
и, соответственно,
x1 = (−0.008 − 0.001 · 9.975, 0.888 + 0.089 · 9.975)T = (0.002, 0)T.
Так как ∥f ′(x1)∥ = 0.036 ⩽ ε, то принимаем xmin = (0.002, 0)T .
Таким образом, опять удалось сделать один большой шаг (рав-
ный в данном случае 9.975) вдоль «дна оврага» и найти точку ми-
нимума с требуемой точностью, решив при этом три задачи од-
номерной оптимизации, против четырех в методе наискорейшего
градиентного спуска.
§ 3.4.
Метод сопряженных направлений
Рассмотрим выпуклую квадратичную функцию q: Rn → R:
q(x) = 1
2xT Ax + bT x + c,
где A – симметричная, положительно определенная матрица,
b = (b1, b2, . . . , bn)T – некоторый вектор, c - константа. Линейно
независимые направления di ∈ Rn, i = 0, . . . , n − 1 называются
32
попарно сопряженными относительно квадратичной формы q(x),
если
dT
i Adi = 0, при i, j = 0, . . . , n − 1, i ̸= j.
Оказывается, если минимизировать q(x) из начальной точки
x0 последовательно вдоль направлений di , то точка минимума бу-
дет найдена не более, чем за n шагов.
Итак, предположим, что точка xk+1 = xk + αkdk , где
αk = argmin
α
q(xk + αdk).
Из последнего условия получаем
d
dαq(xk + αdk) = dT
k q′(xk+1) = dT
k (Axk+1 + b) = 0,
откуда
dT
k (Axk + b) + dT
k Adkαk = 0,
αk = −dT
k (Axk + b)
dT
k Adk
= −
dT
k
�
A
�
x0 +
k−1
�
j=0
αjdj
�
+ b
�
dT
k Adk
.
Поэтому окончательно
αk = −dT
k (Ax0 + b)
dT
k Adk
.
Рассмотрим выражения dT
i (Axk +b) для i = 0, . . . , k−1. Мож-
но аналогично предыдущему показать, что
αi = −dT
i (Ax0 + b)
dT
i Adi
,
то есть
dT
i (Ax0 + b) = −dT
i Adiαi.
Поэтому для любых i = 0, . . . , k − 1 справедливо
dT
i (Axk +b) = dT
i
�
A
�
x0 +
k−1
�
j=0
αjdj
�
+b
�
= dT
i (Ax0 +b)+dT
i Adiαi = 0,
33
то есть
dT
i q′(xk) = 0,
∀i = 0, . . . , k − 1.
(3.2)
Двигаясь из точки
x0
вдоль направлений
di
с шагами
αi ,
i = 0, . . . , k − 1, мы все время остаемся в пределах плоскости, по-
ражденной векторами di и проходящей через точку x0 . Эта плос-
кость по (3.2) ортогональна градиенту функции q в точке xk . Но
этот же градиент ортогонален линии уровня функции q, проходя-
щей через ту же точку xk . Поэтому (см. рассуждения, связанные
с линиями уровня, в конце прошлого параграфа) справедлива
Теорема 3.1.
xk – точка минимума функции q(x) на ли-
нейном подпространстве, порожденном векторами d0, . . . , dk−1 и
проходящем через точку x0 .
В частности, вследствие линейной независимости системы век-
торов {d0, . . . , dn−1}, xn =
n−1
�
j=0
αjdj – точка минимума функции
q(x) на всем пространстве Rn.
Основной вопрос теперь состоит в том, как строить эти сопря-
женные направления. Будем искать их в виде
dk+1 = −q′(xk+1) + βkdk, d0 = −q′(x0),
где βk – некая константа, k < n.
Тогда на первом шаге из того, что d1 сопряжено с d0 , получим
dT
1 Ad0 = 0. Отсюда
(−q′(x1) + β0d0)T Ad0 = 0,
β0 = q′T (x1)Ad0
dT
0 Ad0
.
Покажем,
что
и
в
общем
случае,
если
полагать
dk+1 = −q′(xk+1) + βkdk , и
βk = q′T (xk+1)Adk
dT
k Adk
,
то d0, . . . , dk+1 будут попарно сопряжены.
34
Доказательство будем вести по индукции. При k = 0 утвер-
ждение верно. Пусть оно верно при некотором k − 1, докажем
его справедливость для k. βk определяет dk+1 . Направление dk+1
должно быть сопряжено с dk , поэтому
dT
k+1Adk = −q′T (xk+1)Adk + βkdT
k Adk = 0,
откуда и получаем требуемое представление для βk . Покажем те-
перь, что dk+1 попарно сопряжено с di , i = 0, . . . , k − 1. Для этого
нужно показать, что dT
k+1Adi = 0 при i = 0, . . . , k − 1. Очевидно
dT
k+1Adi = (−q′(xk+1) + βkdk)T Adi = −q′T (xk+1)Adi + βkdT
k Adi.
Второе слагаемое обращается в нуль по индуктивному предполо-
жению, поэтому
dT
k+1Adi = −q′T (xk+1)Adi.
Можем записать
Adi = Axi+1 − xi
αi
= 1
αi
�
(Axi+1 + b) − (Axi + b)
�
,
то есть
Adi = 1
αi
�
q′(xi+1) − q′(xi)
�
.
Но
q′(xi+1) = di+1 + βidi,
q′(xi) = di + βi−1di−1.
Таким
образом,
Adi
–
линейная
комбинация
векторов
di−1, di, di+1 (при i = 0 Ad0 – линейная комбинация d0, d1). На-
правления d0, . . . , dk попарно сопряжены по индуктивному пред-
положению. Тогда по теореме 3.1 вектор q′(xk+1) ортогонален всем
d0, . . . , dk . Поэтому при i = 0, . . . , k − 1
dT
k+1Adi = −q′T (xk+1)Adi =
= − 1
αi
q′(xk+1)T (di+1 + βidi − di − βi−1di−1) = 0.
35
Линейная независимость построенных направлений следует из
теоремы 3.1. Таким образом, наше утверждение доказано.
Перейдем теперь к упрощению формул для βk . Для этого вна-
чале заметим, что
αk = −dT
k q′(xk)
dT
k Adk
= −(−q′(xk) + βk−1dk−1)T q′(xk)
dT
k Adk
= q′T (xk)q′(xk)
dT
k Adk
.
В выражении для βk фигурирует Adk , которое можно запи-
сать в виде
Adk = A
αk
(xk+1 − xk) = 1
αk
(Axk+1 − Axk).
Добавляя и вычетая вектор b получим
Adk = 1
αk
�
q′(xk+1) − q′(xk)
�
= q′(xk+1) − q′(xk)
q′(xk)T q′(xk)
dT
k Adk.
Откуда
βk = q′T (xk+1)Adk
dT
k Adk
= q′(xk+1)T (q′(xk+1) − q′(xk))
q′(xk)T q′(xk)
.
(3.3)
Если вспомнить, что q′(xk) = −dk + βk−1dk−1 , то с учетом ортого-
нальности q′(xk+1) векторам d0, . . . , dk можем переписать (3.3) в
виде:
βk = q′(xk+1)T q′(xk+1)
q′(xk)T q′(xk)
.
(3.4)
Отметим, что в выражениях (3.3), (3.4) матрица A не фигу-
рирует.
Описанный выше подход хорошо работает на выпуклых квад-
ратичных функциях, позволяя минимизировать их за конечно чис-
ло шагов. Ясно, что произвольная функция в некоторой окрестно-
сти минимума может быть аппроксимирована (с помощью разло-
жения в ряд Тейлора) строго выпуклой квадратичной функцией
36
(если выполнены достаточные условия минимума второго поряд-
ка). Поэтому можно рассчитывать, что применение аналогичного
алгоритма к произвольным достаточно гладким функциям тоже
даст хорошие результаты. Итак, пусть задана некоторая гладкая
функция f : Rn → R. Выражения (3.3), (3.4) приводят к двум ме-
тодам:
Метод Флетчера-Ривза.
Выбираем x0 , d0 = −f ′(x0).
1. Пусть имеется xk , dk .
Если k + 1 кратно n, вычисляем αk = argmin
α
f(xk + αdk) и
строим
xk+1 = xk + αkdk,
dk+1 = −f ′(xk+1).
Иначе переходим ко второму пункту.
2. Вычисляем αk = argmin
α
f(xk + αdk).
Строим
xk+1 = xk + αkdk,
βk = ∥f ′(xk+1)∥2
∥f ′(xk)∥2 ,
dk+1 = −f ′(xk+1) + βkdk.
3. Проверяем критерий остановки. Если он выполнен, то закан-
чиваем вычисления. Если нет, то увеличиваем k на единицу
и переходим к первому пункту.
Метод Полака-Рибьера.
Отличается от метода Флетчера-Ривза формулой для нахож-
дения βk во втором пункте. Здесь предлагается использовать вы-
ражение:
βk = f ′T (xk+1)(f ′(xk+1) − f ′(xk))
∥f ′(xk)∥2
.
Первый пункт в этих методах обязывает нас на шагах крат-
ных n начинать построение сопряженных направлений заново (об-
новление метода). При этом условии шаги с номерами nl + 1, где
37
l = 0, 1, . . . совпадают с шагом по МНГС, на промежуточных же
шагах, очевидно, значения функции не возрастают. Отсюда схо-
димость построенных методов будет следовать непосредственно из
сходимости МНГС. Если методы периодически не обновлять, то
накопление погрешностей в процессе расчетов может приводить к
тому, что вектора dj при больших j перестают указывать направ-
ления убывания функции f . Более детальное описание указанных
методов, читатель может найти в [3].
Ясно, что для квадратичной функции методы Флетчера-Ривза
и Полака-Рибьера эквивалентны. Первый шаг по этим методам сов-
падает с шагом наискорейшего градиентного спуска.
Отметим, что хотя изначально (при определении сопряжен-
ных направлений di , i = 0, . . . , n − 1) и использовалась матрица
вторых производных (A), однако в формулы расчета полученных
алгоритмов она не входит. Таким образом, эти методы остаются
методами первого порядка.
Пример. Рассмотрим функцию f(x)
=
9x2
1 + x2
2, выбе-
рем x0 = (1, 1)T
и найдем точку минимума с помощью метода
Флетчера-Ривза. Понятно, что тут он должен привести нас к точ-
ке минимума за два шага. Здесь, в отличие от примера 3.2, будем
вести расчеты с точностью до пяти знаков после запятой, так как
сходимость метода существенно зависит от точности решения зада-
чи одномерной минимизации.
Сделав из
x0
один шаг по методу наискорейшего гра-
диентного спуска, получим d0
=
(−18, −2)T , α0
=
0.05616,
x1 = (−0.01088, 0.88768)T , f ′(x1) = (−0.19584, 1.77536)T ,
β0 = ∥f ′(x1)∥2
∥f ′(x0)∥2 = 0.00961,
d1 = −f ′(x1) + β0d0 = (0.02286, −1.79458)T.
Переходим к следующему
шагу.
x2
=
x1 + α2d1 , где
α2 = argmin
α
f(x1 + αd1). В результате, получаем α = 0.49462,
x2 = (0.00043, 0.00004)T , что очень близко к настоящей точке ми-
нимума (0, 0)T . Чтобы получить более точный результат, нужно
вести расчеты с большей точностью.
38
Перейдем теперь, к методам второго порядка, то есть исполь-
зующим матрицу Гессе – матрицу вторых производных. Ясно, что
ввиду использования большего количества информации, эти мето-
ды должны сходится быстрее методов первого порядка.
§ 3.5.
Метод Ньютона
Пусть функция f дважды непрерывно дифференцируема и
матрица Гессе f ′′(x) – положительно определена для любых x ∈
Rn. Здесь, как и прежде, будем искать точку x∗ , в которой вы-
полнено необходимое условие экстремума f ′(x∗) = 0. Итак, пусть
имеется некоторая точка xk . Рассмотрим разложение градиента в
ряд Тейлора в окрестности этой точки:
f ′(x) = f ′(xk) + ⟨f ′′(xk), x − xk⟩ + o(x − xk),
где
lim
x→xk
∥o(x − xk)∥
∥x − xk∥
= 0.
Идея метода Ньютона заключается в замене функции f ′(x) в
окрестности точки xk ее аппроксимацией f ′(xk) + ⟨f ′′(xk), x − xk⟩.
Точку, в которой эта аппроксимация равна нулю, берут в ка-
честве xk+1 :
xk+1 = xk − [f ′′(xk)]−1f ′(xk).
(3.5)
Итак, задавая произвольную x0 , запускают расчеты по фор-
муле (3.5), проверяя на каждой итерации критерий остановки. Этот
процесс является, очевидно, обобщением на случай функции мно-
гих переменных метода Ньютона-Рафсона, который, как мы виде-
ли, может расходится, когда начальная точка далека от x∗ . Для
того чтобы преодолеть эту проблему, алгоритм (3.5) модифици-
руют. Прежде, чем перейти к описанию этой модификации, отме-
тим, что в силу положительной определенности f ′′(x) направление
pk = −[f ′′(xk)]−1f ′(xk) будет направлением спуска в точке xk , так
как
⟨f ′(xk), pk⟩ = ⟨f ′(xk), −[f ′′(xk)]−1f ′(xk)⟩ < 0.
(3.6)
Таким образом, классический метод Ньютона можно рассматри-
вать как процесс:
39
xk+1 = xk + αkpk,
где αk = 1. В модифицированном же методе Ньютона предлагается
искать αk с помощью решения задачи одномерной минимизации:
αk = argmin
α>0
f(xk − αk[f ′′(xk)]−1f ′(xk)).
Пример. Рассмотрим функцию f(x) = 9x2
1 + x2
2, выберем
x0 = (1, 1)T и найдем точку минимума с помощью модифициро-
ванного метода Ньютона.
Имеем
f ′(x) =
�
18x1
2x2
�
;
f ′′(x) =
�
18
0
0
2
�
;
[f ′′(x)]−1 =
�
1/18
0
0
1/2
�
.
Отсюда
x =
�
1
1
�
− α
�
1/18
0
0
1/2
� �
18
2
�
=
�
1 − α
1 − α
�
,
α0 = argmin
α>0
f(x) = argmin
α>0
�
9(1 − α)2 + (1 − α)2�
= 1.
Поэтому окончательно
x1 =
�1
1
�
.
Задания
1. Почему при применении к строго выпуклой квадратичной
функции метод Ньютона сходится за одну итерацию.
2. Почему для строго выпуклой квадратичной функции в моди-
фицированном методе Ньютона всегда α0 = 1.
3. Покажите, что матрица, обратная к положительно опре-
деленной, так же является положительно определенной
(этот факт использовался в (3.6)).
40
§ 3.6.
Квазиньютоновский метод с поправкой ранга 1
Основной трудностью при реализации метода Ньютона явля-
ется вычисление и обращение матрицы вторых производных. Пусть
необходимо минимизировать функцию методом Ньютона квадра-
тичную функцию
q(x) = 1
2xT Ax + bT x + c,
где A – симметричная, положительно определенная матрица,
b = (b1, b2, . . . , bn)T – некоторый вектор, c - константа. Если каким-
то образом удастся заменить в (3.5) матрицу [q′′]−1 = A−1 на неко-
торую ее аппроксимацию Hk так, чтобы последовательность Hk
за конечное число шагов сходилась к A−1, то последний шаг этого
метода совпадет с шагом по методу Ньютона, который для квад-
ратичной функции сходится за одну итерацию. То есть такой ал-
горитм позволит минимизировать q(x) за конечное число шагов.
Ясно, что применение его к произвольной функции так же должно
давать хорошие результаты, по крайней мере в окрестности точки
минимума (вспомните рассуждения из параграфа 3.4).
Итак, пусть имеется xk , Hk . Будем считать
xk+1 = xk − αkHkf ′(xk), αk = argmin
α>0
f(xk − αkHkf ′(xk)).
Имеем
f ′(xk+1) − f ′(xk) = f ′′(xk)(xk+1 − xk) + o(x − xk).
Отсюда, отбрасывая o(x − xk), можем записать приближенное ра-
венство
[f ′′(xk)]−1(f ′(xk+1) − f ′(xk)) ≈ xk+1 − xk,
которое для квадратичной функции с отличным от нуля гессианом,
очевидно, превращается в точное равенство. Используя это прибли-
женное равенство и информацию о полученной точке xk+1 , можно
определить следующую матрицу Hk+1
Hk+1(f ′(xk+1) − f ′(xk)) = xk+1 − xk.
(3.7)
41
Условие (3.7) называют квазиньютоновским. Обозначим
γk = f ′(xk+1) − f ′(xk), δk = xk+1 − xk.
Тогда (3.7) перепишется в виде
Hk+1γk = δk.
Будем искать Hk+1 в виде
Hk+1 = Hk + Uk,
где матрица Uk – матрица поправок. В зависимости от ее ранга го-
ворят о квазиньютоновских методах с поправками различных ран-
гов (обычно 1 или 2). Будем рассматривать лишь случай поправки
единичного ранга, поэтому примем Uk = uvT , где u и v – n-мерные
вектора-столбцы. Тогда из (3.7) получим
(Hk + uvT )γk = δk,
откуда
u =
1
vT γk
(δk − Hkγk).
(3.8)
Матрица вторых производных симметрична, поэтому будем счи-
тать, что и Hk симметрична, и добиваться симметричности матри-
цы Hk+1 . Для этого нужно, чтобы вектора u и v были коллинеар-
ны, то есть, чтобы vT = λuT , где λ ∈ R.
Hk+1 − Hk = uvT = (δk − Hkγk)vT
vT γk
= (δk − Hkγk)λuT
λuT γk
,
используя представление (3.8) для u, выразим
Hk+1 − Hk = (δk − Hkγk)(δk − Hkγk)T
(δk − Hkγk)T γk
.
В [3] доказана следующая
42
Теорема 3.1. Пусть f квадратичная функция, А — ее мат-
рица Гессе (предполагаемая положительно определенной). Пусть
итерационный процесс с началом в точке x0 при перемещении
вдоль n последовательно независимых направлений δ1, . . . , δn по-
рождает последовательно точки x1 = x0+δ1, . . . , xn = xn−1+δn.
Тогда последовательность матриц Hk :
H0 — произвольная симметричная матрица,
Hk+1 = Hk + (δk − Hkγk)(δk − Hkγk)T
(δk − Hkγk)T γk
,
сходится не более чем за n этапов к обращению A−1 матрицы
Гессе функции f , причем
γk = f ′(xk+1) − f ′(xk) = A(xk+1 − xk) = Aδk.
Итак, окончательно сформулируем алгоритм:
Выбираем произвольную точку x0 , H0 = E (E – единичная
матрица).
1. Пусть заданы xk , Hk . Вычисляем
xk+1 = xk − αkHkf ′(xk),
где αk = argmin
α>0
f(xk − αkHkf ′(xk)).
2. Если k + 1 кратно n, то Hk+1 = E , иначе
Hk+1 = Hk + (δk − Hkγk)(δk − Hkγk)T
(δk − Hkγk)T γk
.
3. Проверяем критерий остановки: если выполнен, то расчеты
заканчиваются и x∗ = xk+1 , если нет, то увеличиваем k на
единицу и переходим к пункту 1.
При этом условии шаги с номерами nl+1, где l = 0, 1, . . . сов-
падают с шагом по МНГС, на промежуточных же шагах, очевидно,
значения функции не возрастают. Отсюда сходимость описанного
метода будет следовать непосредственно из сходимости МНГС.
43
Более подробно с классом квазиньютоновских методов чита-
тель может ознакомиться, например, в [5].
§ 3.7.
Численный эксперимент
В заключении второй главы применим изученные методы для
минимизации функции Розенброка
f(x) = 100(x2 − x2
1)2 + 5(1 − x1)2.
Очевидно минимальное значение этой функции равно нулю и
достигается в точке (1, 1)T . Возьмем x0 = (0, 0)T , ε = 0.003, оста-
навливать вычисления будем тогда, когда норма градиента станет
меньше ε. В результате получим
Название метода
время, с
ит-ий
x∗
f(x∗)
ГС с дроб-ем шага
0.57
731
�
0.9905
0.9806
�
4.77e-04
МНГС
23.14
296
�0.9994
0.9987
�
2.02e-06
Ускор. спуск 2-порядка
85.47
138
�
0.9994
0.9988
�
1.76e-06
Метод Флетчера-Ривза
1.79
11
�0.9999
0.9998
�
5.9e-08
Метод Ньютона
1.83
9
�0.9999
0.9999
�
2.4e-08
Везде, где необходима одномерная минимизация, она проводи-
лась с помощью метода пассивного поиска с точностью 0.00001. В
градиентном методе с дроблением шага, дробление шага проводи-
лось с помощью δ = 0.9. Как видим, этот метод привел к решению
быстрее всего. Дело в том, что при его реализации, в отличие от
других методов, не производилась одномерная оптимизация. Вре-
мя расчетов по остальным методам можно значительно сократить,
если использовать более совершенные алгоритмы для одномерной
минимизации.
Ускоренному градиентному спуску второго порядка понадоби-
лось меньше итераций, чем МНГС, но он работал дольше послед-
него.
44
В методе Флетчера-Ривза «обнуление» проводилось на номе-
рах итераций, кратных трем. Этот метод сошелся быстрее метода
Ньютона, но потребовал больше итераций.
Наглядно демонстрируют работу использованных методов
следующие иллюстрации*:
Рисунок 3.8: Линии уровня с траекториями движения к минимуму
по методу Ньютона.
а)
б)
*Расчеты и построения графиков выполнены в математическом пакете
Matlab студентом Ивановым Н.Г.
45
в)
г)
Рисунок 3.9: Линии уровня с траекториями движения к минимуму
соответствующие:
а) Градиентному спуску с дроблением шага,
б) Наискорейшему градиентному спуску,
в) Ускоренному градиентному спуску второго порядка,
г) Методу Флетчера-Ривза.
46
ГЛАВА 4.
УСЛОВНАЯ ОПТИМИЗАЦИЯ
В главе 3 рассматривалась задача безусловной оптимизации,
то есть оптимизации функции на всем пространстве. Эта глава по-
священа условной оптимизации, то есть решению задачи
�
f(x) −→ inf,
x ∈ X
где X – некоторое непустое множество, называемое допустимым.
Причем будем считать, что X определяется ограничениями в виде
неравенств и равенств нулю некоторых достаточно гладких функ-
ций.





f(x) −→ inf,
gi(x) ⩽ 0, i = 1, . . . , m
hj(x) = 0, j = 1, . . . , l
(4.1)
Ясно, что тут допустимое множество
X = {x ∈ Rn | gi(x) ⩽ 0, hj(x) = 0, i = 1, . . . , m, j = 1, . . . , l}.
Если ограничения-неравенства в некоторой точке x обраща-
ются в равенства, то соответствующие ограничения называют ак-
тивными в данной точке, а совокупность всех таких индексов R(x)
– множеством индексов активных ограничений
R(x) =
�
i ∈ {1, . . ., m}
�� gi(x) = 0
�
.
§ 4.1.
Условия Куна-Таккера
Методы безусловной оптимизации базировались на необходи-
мом условиях экстремума (равенство градиента нулю), поэтому
вначале сформулируем (см. [3], [6]) необходимые условия экстре-
мума для задачи (4.1).
Теорема 4.1. [Куна — Таккера] Пусть x∗ ∈ X и градиен-
ты g′
i(x∗), h′
j(x∗) при i ∈ R(x∗), j = 1, . . . , l образуют линейно
независимую систему векторов, тогда для того, чтобы точка x∗
47
была решением задачи (4.1) необходимо, чтобы нашлись λi ⩾ 0,
i = 1, . . . , m и µj , j = 1, . . . , l произвольного знака, такие что







f ′(x∗) +
m
�
i=1
λjg′
i(x∗) +
l
�
j=1
µjh′
j(x∗) = 0,
λig(x∗) = 0, i = 1, . . . , n.
Аналогично можно сформулировать эти условия и для част-
ных случаев задачи (4.1).
�
f(x) −→ min,
gi(x) ⩽ 0, i = 1, . . . , m.
(4.2)
Тут допустимое множество
X = {x ∈ Rn | gi(x) ⩽ 0, i = 1, . . . , m}.
Теорема 4.2. [Куна — Таккера] Пусть x∗ ∈ X и градиен-
ты g′
i(x∗) при i ∈ R(x∗) образуют линейно независимую систему
векторов, тогда для того чтобы точка x∗ была решением задачи
(4.2) необходимо, чтобы нашлись λi ⩾ 0, i = 1, . . . , m, такие что





f ′(x∗) +
m
�
i=1
λjg′
i(x∗) = 0,
λig(x∗) = 0, i = 1, . . . , n.
Если же все ограничения имеют вид равенств, то приходим к
известным из курса математического анализа условиям.
�
f(x) −→ min,
hj(x) = 0, j = 1, . . . , l.
(4.3)
Тут допустимое множество
X = {x ∈ Rn | hj(x) = 0, j = 1, . . . , l}.
Теорема 4.3. [Куна — Таккера] Пусть x∗ ∈ X и градиенты
h′
j(x∗) при j = 1, . . . , l образуют линейно независимую систему
48
векторов, тогда для того чтобы точка x∗ была решением задачи
(4.3) необходимо, чтобы нашлись µj , j = 1, . . . , l, такие что
f ′(x∗) +
l
�
j=1
µjh′
j(x∗) = 0,
Пример. Рассмотрим простой пример
�
f(x) = x2 −→ min,
g(x) = x − 1 ⩽ 0.
Ясно, что минимум здесь достигается в нуле. Попробуем получить
эту точку с помощью условий Куна-Таккера. Выписывая эти усло-
вия, получаем





2x + λ = 0,
λ(x − 1) = 0,
λ ⩾ 0.
Здесь возможны два случая





2x + λ = 0,
λ = 0,
x − 1 ⩽ 0,
либо





2x + λ = 0,
x − 1 = 0,
λ ⩾ 0.
В первой из этих двух систем условие x − 1 ⩽ 0 необходимо,
чтобы точка оставалась допустимой (принадлежала допустимому
множеству X ). Множество решений второй системы пусто. Из пер-
вой же получаем x = 0, λ = 0.
Пример. Рассмотрим
�
f(x) = x2
1 + x2
2 −→ min,
g(x) = 2x1 + x2 + 4 ⩽ 0.
Выписываем условия Куна-Таккера





2x1 + 2λ = 0,
2x2 + λ = 0,
λ(2x1 + x2 + 4) = 0.
Здесь возможны два случая
49









2x1 + 2λ = 0,
2x2 + λ = 0,
λ = 0,
2x1 + x2 + 4 ⩽ 0,
либо









2x1 + 2λ = 0,
2x2 + λ = 0,
λ ⩾ 0,
2x1 + x2 + 4 = 0.
Множество решений первой системы пусто. Из второй же по-
лучаем
x1 = −8
5 = −1.6, x2 = −4
5 = −0.8, λ = 8
5 = 0.8.
Понятно, что при применении условий Куна-Таккера к зада-
че, содержащей k ограничений-неравенств, нужно рассмотреть 2k
систем.
§ 4.2.
Метод внешних штрафов
Если взять функцию
H(x) =
�
0,
x ∈ X,
+∞,
x /∈ X,
то задачу условной оптимизации (4.1) можно заменить на задачу
безусловной оптимизации функции f(x) + H(x), так как
f(x) + H(x) =
�
f(x),
x ∈ X,
+∞,
x /∈ X,
поэтому минимум f(x)+H(x) на всем пространстве совпадает с ми-
нимумом f(x) на X . Проблемой на этом пути является отсутствие
непрерывности у построенной таким образом функции f(x)+H(x).
Поэтому H(x) аппроксимируют непрерывной функцией, которая
принимает положительные значения при x /∈ X и равна нулю при
x ∈ X . То есть x штрафуется за выход из допустимого множе-
ства. Методы, реализующие эту идею, называют методами внешних
штрафов.
Рассмотрим
ϕ(x, r) = f(x) + rH(x),
50
где r > 0– скаляр, называемый коэффициентом штрафа. С од-
ной стороны, желательно, чтобы r было велико, потому что то-
гда точка минимума этой функции будет близка к решению (4.1).
С другой стороны, при больших r матрица вторых производных
функций ϕ(x, r) становится плохо обусловленной и, следователь-
но, она будет иметь линии уровня сильно овражного вида, что мо-
жет привести к плохой сходимости оптимизационных алгоритмов.
Пример, иллюстрирующий этот эффект, приведен в [7]. Кроме то-
го, вследствие плохой обусловленности будет наблюдаться эффект
неустойчивости – даже небольшие погрешности в вычислениях (а
они неизбежны, так как любые расчеты ведутся с точностью до
определенного знака после запятой) могут приводить к большим
отклонениям в {xk}. Если начальная точка x0 находится далеко
от точки безусловного минимума ϕ(x, r), это приводит к увеличе-
нию вероятности прерывания работы алгоритма вследствие потери
точности.
Для преодоления обозначенных трудностей на практике ис-
пользуют идею постепенного увеличения параметра штрафа. При-
чем на каждой итерации в качестве начальной берут точку, полу-
ченную на предыдущем шаге. Это позволяет каждый раз запус-
кать алгоритм безусловной оптимизации из некоторой окрестности
точки минимума. Итак, берут некоторую последовательность {rk},
rk > 0, rk+1 > rk , k ∈ N, полагают x(r0) = x0 , выбирают некоторое
ε > 0 и далее
• x0 = x(rk−1), x(rk) = argmin
x∈Rn (f(x) + rkH(x));
• если H(x(rk)) < ε (то есть x(rk) с точностью ε принадлежит
допустимому множеству), то останавливаются и принимают
x∗ = x(rk). Если же это неравенство не выполняется увели-
чивают k на единицу и возвращаются к предыдущему пункту.
Таким образом, задача условной оптимизации сводится к по-
следовательности задач безусловной оптимизации.
Можно показать (см., например [3]), что метод внешних штра-
фов сходится к оптимальному решению при достаточно слабых
ограничениях.
51
Теорема. Пусть H : Rn → R — функция внешнего штрафа,
удовлетворяющая условиям
⋄ H(x) > 0 ∀x,
⋄ H(x) = 0 ⇔ x ∈ X,
⋄ H непрерывна.
Пусть f —непрерывная функция, множество X замкнуто и
выполняется одно из двух условий:
1) f(x) → +∞ при ∥x∥ → +∞;
2) X ограничено и H(x) → +∞ при ∥x∥ → +∞.
Тогда при r → +∞:
a) последовательность x(r) имеет по крайней мере одну пре-
дельную точку и всякая точка сгущения этой последовательно-
сти есть (глобальное) оптимальное решение задачи;
b) H(x(r)) → 0.
Укажем один из возможных видов функции H(x), которым
будем пользоваться в дальнейшем:
H(x) =
m
�
i=1
[max(0, gi(x))]2 +
l
�
j=1
hj(x)2.
Эта функция непрерывно дифференцируема. Действительно, для
каждого слагаемого из первой суммы можем записать
[max(0, gi(x))]2 =
�
g2
i (x),
при gi(x) ⩾ 0,
0,
при gi(x) < 0,
откуда
�
[max(0, gi(x))]2�′ =
�
2g′
i(x)gi(x),
при gi(x) ⩾ 0
0,
при gi(x) < 0 =
= 2g′
i(x)
�
gi(x),
при gi(x) ⩾ 0
0,
при gi(x) < 0 = 2g′
i(x) max(0, gi(x)).
Поэтому
H′(x) =
m
�
i=1
2g′
i(x) max(0, gi(x)) +
l
�
j=1
2h′
j(x)hj(x).
52
Таким образом, для решения промежуточных задач безуслов-
ной оптимизации с такой H(x) можно использовать любой алго-
ритм нулевого или первого порядка.
Пример. Вернемся к примеру, рассмотренному в прошлом
пункте
�
f(x) = x2 −→ min,
g(x) = x − 1 ⩽ 0.
Пусть x0 = 5, r1 = 1, rk+1 = 10rk , ε = 0.01. Составляем функцию
H(x) = [max(0, x − 1)]2 .
На первом шаге нужно минимизировать функцию
ϕ(x, r1) = x2 + [max(0, x − 1)]2,
взяв в качестве начальной точки x0 = 5. Подставив в ϕ(x, r1)
x = x0 + α = 5 + α, получим гладкую строго выпуклую функцию
ϕ(α, r1) = (5 + α)2 + [max(0, 4 + α)]2,
поэтому ее минимум находится из уравнения
ϕ′(α, r1) = 2(5 + α) + 2[max(0, 4 + α)] = 0.
Здесь может быть два случая
�
α ⩾ −4
2(5 + α) + 2(4 + α) = 0
либо
�
α < −4
2(5 + α) = 0
Множество решений первой системы пусто. Из второй же по-
лучаем α = −5, откуда x(r1) = 0. Критерий остановки выполнен,
так как H(x(r1)) = 0. Поэтому x∗ = 0.
Пример. Вернемся к задаче
�
f(x) = x2
1 + x2
2 −→ min,
g(x) = 2x1 + x2 + 4 ⩽ 0.
Возьмем x0 = (0, 0)T , r1 = 1, rk+1 = 10rk , ε = 0.01. Задачи без-
условной оптимизации на каждом шаге будем решать с точностью
ε1 = 0.001. Имеем
ϕ(x, r1) = f(x) + H(x) = x2
1 + x2
2 + [max(0, 2x1 + x2 + 4)]2, x10 = x0.
53
Здесь xij обозначает точку, полученную на j -ой итерации i-ой за-
дачи безусловной оптимизации, каждую из которых будем решать с
помощью метода наискорейшего градиентного спуска, взяв в каче-
стве критерия остановки условие малости нормы градиента. Везде
далее в целях экономии места приведены значения до шестого знака
после запятой, хотя расчеты велись до пятнадцатого знака. Итак,
ϕ′(x, r1) =
�
2x1
2x2
�
+ 2 max(0, 2x1 + x2 + 4)
�
2
1
�
.
Отсюда
x11 = x10 − α0ϕ′(x0, r1) =
�
1 − 30α0
1 − 16α0
�
,
где α0 = argmin ϕ(α, r1),
ϕ(α, r1) = (1 − 30α)2 + (1 − 16α)2 + [max(0, 7 − 76α)]2,
поэтому
α0
=
0.083381,
x11
=
(−1.501443, −0.334103)T ,
∥ϕ′(x11, 1)∥ = ∥(−0.350837, 0.657819)T∥ > ε1 . Продолжая аналогич-
но, далее получаем
x12 =
�−1.326629
−0.661878
�
, ϕ′(x12, 1) =
� 0.086197
0.0045972
�
, ∥ϕ′(x12, 1)∥ > ε1,
x13 =
�
−1.333816
−0.665711
�
, ϕ′(x13, 1) =
�
−0.001008
0.00189
�
, ∥ϕ′(x13, 1)∥ > ε1,
x14 =
�−1.333314
−0.666653
�
, ϕ′(x14, 1) =
�0.000245
0.000131
�
, ∥ϕ′(x14, 1)∥ < ε1.
Таким образом, x(r1) = x14 , но
H(x(r1)) = 0.444512 > ε,
ввиду чего нужно продолжить вычисления, перейдя ко второй
итерации.
Принимаем x20 = x(r1) и строим функцию
ϕ(α, r2) = f(x) + 10H(x) = x2
1 + x2
2 + 10[max(0, 2x1 + x2 + 4)]2.
54
x21 =
�−1.568629
−0.784310
�
, ϕ′(x21, 1) =
�0.000003
0.000007
�
, ∥ϕ′(x21, 10)∥ < ε1.
Поэтому x(r2) = x21 . Проверив критерий остановки
H(x(r2)) = 0.006151 < ε,
заключаем, что x∗ = x(r2).
В заключении этого пункта укажем на связь метода внеш-
них штрафов и условий Куна-Таккера. Для задачи типа 4.2, ко-
торая рассматривалась в примерах, эти условия означают, что
−f ′(x∗) раскладывается по градиентам активных ограничений
g′
i(x∗), i ∈ R(x∗). Причем в силу их линейной независимости, та-
кое разложение единственно. При выполнении условий теоремы 4.2
можно без ограничения общности считать, что x(rk) → x∗ при
k → ∞. Рассмотрим некоторое j /∈ R(x∗). Имеем gj(x∗) < 0, по-
этому в силу непрерывности
∃Nj > 0 : ∀k ⩾ Nj выполнено gj(x(rk)) < 0.
Взяв N =
max
j /∈R(x∗) Nj , получим
gj(x(rk)) < 0 то есть max(0, gj(x(rk))) = 0
∀j /∈ R(x∗).
Точка x(rk) должна удовлетворять необходимому условию экстре-
мума ϕ′(x(rk), rk) = 0, то есть
f ′(x(rk)) +
n
�
i=1
2rkg′(x) max(0, gi(x(rk))) = 0,
которое при k > N превратится в
f ′(x(rk)) +
�
i∈R(x∗)
2rkg′(x(rk)) max(0, gi(x(rk))) = 0.
Отсюда, ввиду единственности разложения антиградиента f по
градиентам активных ограничений, приходим к выводу о том, что
2rk max(0, gi(x(rk))) стремятся при k → ∞ к оптимальным множи-
телям Куна-Таккера λi .
55
Действительно, в последнем примере
2r2 max(0, g(x(r2))) = 1.56864,
что близко к истинному значению 1.6 оптимального множителя λ.
§ 4.3.
Метод внутренних штрафов
Основное неудобство метода внешних штрафов связано с тем,
что он генерирует последовательность точек x(rk), не принадле-
жащих допустимому множеству. То есть приближение к решению
нашей задачи происходит извне допустимой области (см. второй
пример прошлого пункта). Поэтому в [8] был предложен другой
вариант метода штрафов, который выдает последовательность до-
пустимых точек, то есть аппроксимирует оптимум изнутри допу-
стимой области.
Итак, пусть
• X имеет непустую внутренность;
• каждая граничная точка множества X есть предел последо-
вательности точек, лежащих внутри X .
Функцией внутреннего штрафа или барьерной функцией, бу-
дем называть функцию B(x), которая
• неотрицательна в любой внутренней точке допустимого мно-
жества X ;
• стремится к +∞ при приближении x к границе X .
Далее рассмотрим функцию
ψ(x, t) = f(x) + tB(x),
где t > 0 – коэффициент штрафа. Выбираем некоторое t1 > 0,
x10 = x0 ∈ intX , ε > 0, ε1 > 0.
1. Пусть заданы tk > 0, xk0 . Решаем задачу безусловной оп-
тимизации ψ(x, tk) из начальной точки xk0 с точностью ε1. Полу-
ченное решение обозначаем x(tk). B(x) создает барьер на границе
56
допустимого множества (ψ неограниченно возрастает при прибли-
жении к границе), поэтому всегда будем находится в пределах X .
2. Проверяем условие tkB(x(tk)) < ε. Если оно выполнено, то
принимаем x∗ = x(tk), если нет, то выбираем новый коэффициент
штрафа 0 < tk+1 < tk , новую начальную точку x(k+1)0 = x(tk),
увеличиваем k на единицу и возвращаемся к первому пункту.
Условия сходимости указанного алгоритма дает
Теорема. Пусть X = {x| gi(x) ⩽ 0}, i = 1, .., m – допу-
стимое множество задачи (4.2). Предположим, что X замкну-
то, имеет непустую внутренность и что любая точка x ∈ X
есть предел последовательности точек, принадлежащих внут-
ренности X . Пусть B(x): Rn → R – функция внутреннего штра-
фа, удовлетворяющая следующим условиям:
⋄ B(x) > 0 ∀x ∈ intX ;
⋄ B(x) → +∞ при x, стремящемся к границе X ;
⋄ B(x) непрерывна на intX .
Предположим, с другой стороны, что f – непрерывная функ-
ция и выполнено хотя бы одно из следующих двух условий:
1) f(x) → +∞ при ∥x∥ → +∞;
2) X ограничено.
Тогда при коэффициенте штрафа t, стремящемся к нулю,
имеем:
– последовательность x(t) имеет по крайней мере одну пре-
дельную точку, и каждая предельная точка последовательности
x(t) есть (глобальный) оптимум задачи (4.2);
– величина tB(x(t)) стремится к нулю.
Можно взять, например
B(x) = −
m
�
j=1
1
gi(x).
Необходимо отметить, что в ходе одномерной минимизации
мы, вообще говоря, можем выйти за границы допустимой области.
Поэтому нужно постоянной контролировать получаемые точки на
допустимость и на уровне одномерной оптимизации предусмотреть
специальные процедуры, гарантирующие, что очередная точка не
выйдет из допустимого множества. В этом недостаток рассматри-
ваемого метода по сравнению с методом внешних штрафов. Кроме
57
того, процесс расчетов нужно начинать из точки, принадлежащей
внутренности допустимой области, однако поиск такой точки не
всегда является тривиальной задачей.
Пример. Рассмотрим, уже знакомый пример
�
f(x) = x2 −→ min,
g(x) = x − 1 ⩽ 0.
Возьмем x0 = −5, x0 ∈ intX , t1 = 1, tk+1 = tk/10, ε = 0.01,
ε1 = 0.001.
Итак,
ψ(x, tk) = x2 − tk
1
x − 1.
Для решения задач безусловной оптимизации выберем метод наи-
скорейшего градиентного спуска, а а качестве критерия остановки
– малость нормы градиента.
ψ(x, 1) = x2 −
1
x − 1, ψ′(x, 1) = 2x +
1
(x − 1)2 .
x10 = x0 = −5, x11 = x10 − α0ψ′(x10, 1), где
α0 = argmin
α
ψ(x10 − αψ′(x10, 1), 1),
откуда
α0 = argmin
α
(−5 + 9.972222α)2 −
1
−6 + 9.972222α = 0.471594,
а x11 = −0.297157 ∈ intX . Критерий остановки для вспомога-
тельной задачи безусловной оптимизации |ψ(x11, 1)| = 10−6 < ε1
выполнен, значит
x(t1)
=
x(1)
=
−0.297157. Критерий же
остановки основного алгоритма (метода внутренних штрафов)
|t1B(x(t1))| = 0.770917 > ε не выполнен, поэтому переходим ко
второй итерации.
Решаем задачу
ψ(x, 1
10) = x2 −
1
10(x − 1) → min,
x20 = x(1),
58
с точностью ε1 . Получаем x(t2) = x(0.1) = −0.045723 ∈ intX ,
|t2B(x(t2))| = 0.095628 > ε.
Действуя аналогично далее решаем
ψ(x, 1
100) = x2 −
1
100(x − 1) → min,
x30 = x(0.1).
В результате x(t3) = x(0.01) = −0.004951 ∈ intX , |t3B(x(t3))| =
0.009951 < ε, откуда x∗ = x(t3) = −0.004951.
Отметим в заключении, что проводя рассуждения, аналогич-
ные таковым в конце прошлого пункта, можно и здесь показать,
стремление определенного выражения у оптимальным множителям
Куна-Таккера:
tk
�
1
gi(x(tk))
�2
→ λi, при k → ∞.
(4.4)
Действительно,
в
рассмотренном
только
что
примере
t3
�
1
x(t3)−1
�2
=
0.0099002, что близко к истинному нулевому
значению.
Подробнее о методах штрафа можно почитать, например в
[9],[10].
Задания
1. Покажите справедливость (4.4).
§ 4.4.
Метод условного градиента
Рассмотрим задачу выпуклого программирования
�
f(x) −→ inf,
x ∈ X.
(4.5)
где f : Rn → R – выпуклая функция, минимизируемая на замкну-
том выпуклом множестве X ⊂ Rn из начальной точки x0 ∈ X .
Попробуем модифицировать градиентный метод в данном случае
так, чтобы он учитывал ограничения (x ∈ X ).
59
Пусть задана точка xk ∈ X . Разложение f в окрестности xk
f(x) = f(xk) + ⟨f ′(xk), x − xk⟩ + o(x − xk).
Для линейной части приращения fk(x) = ⟨f ′(xk), x − xk⟩ с точно-
стью до o-малого можем записать fk(x) = f(x) − f(xk). Поэтому
вначале вместо задачи (4.5) предлагается рассмотреть задачу
�
fk(x) −→ inf,
x ∈ X.
(4.6)
Так как fk(x) – линейная функция, то решение задачи (4.6),
которое обозначим xk , будет лежать на границе X .
Если xk = xk , это означает, что в любых направлениях, ис-
ходящих из xk и не выводящих при достаточно малых шагах из
допустимого множества X , функция f возрастает. Следовательно,
в этом случае xk и будет решением задачи (4.5).
Если же
xk
̸=
xk , то можем рассмотреть направление
pk = xk − xk . Вдоль этого направления убывает линейная часть
приращения fk(x), а значит обязательно найдется такое малое по-
ложительное α, что f(xk + αpk) < f(xk). Кроме того, в силу вы-
пуклости X и принадлежности xk и xk допустимому множеству
X , при α ∈ [0, 1] получим
xk + αpk = xk + α(xk − xk) = αxk + (1 − α)xk ∈ X.
Таким образом, можно найти такое αk , что
f(xk + αkpk) − f(xk) < 0, xk + αkpk ∈ X.
На практике αk ищут обычно одним из двух способов:
1. αk = min{1, α∗}, где α∗ = argmin
α>0
f(xk + αpk);
2.
(a) принимают �α = 1, выбирают δ ∈ (0, 1),
(b) проверяют f(xk + αpk) − f(xk) < 0,
(c) если это условие выполнено, то полагают αk = �α, ес-
ли нет, то дробят �α с помощью δ (то есть принимают
�α = δ�α) и возвращаются к шагу (a).
60
Процесс продолжают до тех пор, пока не выполнится кри-
терий остановки, в качестве которого можно, например, взять
∥f ′(xk)∥ < ε, где ε > 0 – точность решения.
Отметим, что в случае неограниченности множества X можно
искать решение
�
f(x) −→ inf
x ∈ �
X
(4.7)
где �
X = X �{x ∈ Rn| f(x) ⩽ f(x0)} – выпуклое замкнутое множе-
ство, как пересечение двух выпуклых замкнутых множеств.
Если X – параллелепипед или шар, то вспомогательная зада-
ча (4.6) решается просто.
Пусть X = {x ∈ Rn| ai ⩽ xi ⩽ bi, i = 1, . . . , n}, тогда
xk = (xk
1, . . . , xk
n), где
xk
i =







ai, если ∂f(xk)
∂xi
⩾ 0
bi, если ∂f(xk)
∂xi
< 0
(4.8)
Пусть
X = {x ∈ Rn|∥x − a∥ ⩽ r}
шар радиуса r с центром в точке a, тогда
xk = a − r f ′(xk)
∥f ′(xk)∥.
(4.9)
Действительно,
⟨f ′(xk), x − xk⟩ = ⟨f ′(xk), x − a⟩ + ⟨f ′(xk), a − xk⟩,
второе слагаемое – фиксированное число, первое же принимает
минимальное
значение
(см.
неравенство
Коши-Буняковского)
тогда и только тогда, когда вектор x − a коллинеарен f ′(xk),
направлен противоположно ему и равен r по норме.
61
Пример. Пусть нужно решить задачу





x2
1 + x2
2 −→ min
−1 ⩽ x1 ⩽ 2
−2 ⩽ x2 ⩽ 1
и x0 = (1, −1)T – начальная точка.
Имеем f ′(x0) = (2, −2)T . Составляем вспомогательную задачу





2(x1 − 1) − 2(x2 + 1) −→ min,
−1 ⩽ x1 ⩽ 2,
−2 ⩽ x2 ⩽ 1.
Из (4.8) следует, что x0 = (−1, 1)T . Далее ищем точку x1 в виде
x0 + α(x0 − x0) =
�
1
−1
�
+ α
�
−2
2
�
=
�
1 − 2α
−1 + 2α
�
,
откуда
α0 = argmin
α
�
(1 − 2α)2 + (−1 + 2α)2�
= 1
2 < 1.
Поэтому x1 = (0, 0)T . Ввиду того, что ∥f ′(x1)∥ = ∥(0, 0)T∥ = 0,
получим x∗ = x1 .
Пример. Пусть нужно решить задачу
�
x2
1 + x2
2 −→ min,
(x1 − 2)2 + (x2 − 2)2 ⩽ 8,
и x0 = (2, 2)T – начальная точка.
Имеем f ′(x0) = (4, 4)T . Составляем вспомогательную задачу
�
4(x1 − 2) + 4(x2 − 2) −→ min,
(x1 − 2)2 + (x2 − 2)2 ⩽ 8.
62
Из (4.9) следует, что
x0 =
�2
2
�
− 2
√
2
�2/
√
2
2/
√
2
�
=
�0
0
�
.
Далее ищем точку x1 в виде
x0 + α(x0 − x0) =
�2
2
�
+ α
�−2
−2
�
=
�2 − 2α
2 − 2α
�
,
откуда
α0 = argmin
α
�
(2 − 2α)2 + (2 − 2α)2�
= 1.
Поэтому x1 = (0, 0)T . Ввиду того, что ∥f ′(x1)∥ = ∥(0, 0)T∥ = 0,
получим x∗ = x1 .
Задания
1. Покажите, что если f – выпуклая функция, то множество
{x ∈ Rn| f(x) ⩽ f(x0)} также выпуклое.
2. Убедитесь в том, что решения (4.7) и (4.5) совпадают.
3. Может ли и в каких случаях α∗ (см. первый способ опреде-
ления αk ) получиться равным нулю?
4. Подумайте, почему выбор αk = min{1, α∗} (при α∗ ̸= 0) га-
рантирует выполнение неравенства f(xk + αkpk) < f(xk).
Автор надеется, что материал пособия был полезен и ин-
тересен читателю. Предложения и пожелания направляйте по
адресу: majid.abbasov@gmail.com
63
ЛИТЕРАТУРА
1. Васильев Ф.П. Методы оптимизации. М.: Факториал Пресс,
Гл. ред. физ.-мат. лит., 2002.– 824 c.
2. Пшеничный Б.Н., Данилин Ю.М. Численные методы в экс-
тремальных задачах. М.: Наука, 1975. – 320 с.
3. Мину М.
Математическое программирование. Теория и ал-
горитмы. М.: Наука. Гл. ред. физ.-мат. лит., 1990. 488 c.
4. Моисеев Н.Н., Иванилов Ю.П., Столярова Е.М. Методы оп-
тимизации. М.: Наука. Гл. ред. физ.-мат. лит., 1978. 352 c.
5. Гилл Ф., Мюррей У., Райт М.
Практическая оптимизация.
Пер. с англ. — М.: Мир, 1985. 509 c.
6. Аоки М.
Введение в методы оптимизации. М.: Наука, 1977.
334 с.
7. Сухарев А. Г., Тимохов А. В., Федоров В. В. Курс методов
оптимизации: Учеб. пособие. — 2-е изд. — М.: ФИЗМАТЛИТ,
2005. 368 с.
8. Фиакко А., Мак-Кормик Г.
Нелинейное программирова-
ние. Методы последовательной безусловной минимизации. М.:
Мир, 1972. 240 с.
9. Евтушенко Ю.Г.
Методы решения экстремальных задач и
их применение в системах оптимизации. М.: Наука, 1982. 432
с.
10. Химмельблау Д. Прикладное нелинейное программирование.
М.: Мир, 1975. 536 c.
64
